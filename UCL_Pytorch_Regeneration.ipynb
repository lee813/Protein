{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UCL Pytorch Regeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNQsp7Y91oPV+cQyvFfu5bs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee813/Protein/blob/master/UCL_Pytorch_Regeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOsdct8b1q0l",
        "colab_type": "text"
      },
      "source": [
        "# Load dataset from Google cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XToYqmMk1pne",
        "colab_type": "code",
        "outputId": "8396063d-331c-410e-f3f6-2c907e94c028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdYF6wQLzDcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# Imports\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "        \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Switches for what you want the model to do\n",
        "# =============================================================================\n",
        "cuda=True       # for training with gpu, make it true. For inference with cpu make false\n",
        "load=False        # load in the model (default provided is 16 dimensional for nostruc data)\n",
        "train=True      # Make true to train the model presuming you have the dataset\n",
        "new_metal=True   # Make true to produce 'batch_size' samples of a given protein\n",
        "                     # see the docs on github for description of how to do this\n",
        "\n",
        "# trypsins = np.load('/content/drive/My Drive/Protein Data/trypsin140.npy', allow_pickle = True)\n",
        "# print(trypsins.shape)\n",
        "serine = np.load('/content/drive/My Drive/Protein Data/3.4.21_200.npy', allow_pickle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udaKUFs9y5dB",
        "colab_type": "code",
        "outputId": "d1319ca8-feb0-42c3-e115-ad5aec2c02a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(serine.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41386, 4400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E0zT_y5zb0q",
        "colab_type": "text"
      },
      "source": [
        "# Model specification\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il8FjAKhza0t",
        "colab_type": "code",
        "outputId": "c3e10c14-7ac9-4d5f-ff75-b9ad31a8a198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "args_dict = {\n",
        "    \"batch_size\": 256,\n",
        "    \"num_epochs\": 1000,\n",
        "    \"latent_dim\": 16,\n",
        "    \"device\": 0,\n",
        "    \"dataset\": \"nostruc\",\n",
        "    \"lr\": 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "X_dim=4402\n",
        "protein_dim = 4400\n",
        "\n",
        "if cuda:\n",
        "    # if args_dict[\"dataset\"]==\"nostruc\":    \n",
        "    #     data = np.load('/content/drive/My Drive/Protein Data/assembled_data_mb.npy')\n",
        "       \n",
        "    # else:\n",
        "    #     data = []\n",
        "    t3 = np.zeros((41386, 2))\n",
        "    t4 = np.concatenate((serine, t3), axis = 1)\n",
        "    data = t4\n",
        "\n",
        "    print(data.shape)\n",
        "\n",
        "    data, data_test=tts(data, test_size=0.15, shuffle=True)\n",
        "\n",
        "    \n",
        "    # np.concatenate((data, t4), axis = 0)\n",
        "    n=data.shape[0]\n",
        "else:\n",
        "    print(\"No DATA\")\n",
        "    if args_dict[\"dataset\"]==\"nostruc\":\n",
        "        X_dim=4402\n",
        "    else:\n",
        "        X_dim=4353\n",
        "\n",
        "\n",
        "if cuda:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args_dict['device'])\n",
        "\n",
        "#spec batch size\n",
        "batch_size=args_dict['batch_size']\n",
        "#learning rate\n",
        "lr=args_dict['lr']\n",
        "# layer sizes\n",
        "hidden_size=[512,256,128,args_dict['latent_dim']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41386, 4402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAF7-sW531Jq",
        "colab_type": "code",
        "outputId": "dfcb4e9d-f0e0-4310-a504-47f1cc1578a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data_test[4][0:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPC6qbsSzvnM",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh2o9YJGzu8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# Module\n",
        "# =============================================================================\n",
        "class feed_forward(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, batch_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.batch_size = batch_size\n",
        "           \n",
        "\n",
        "        self.fc = torch.nn.Linear(input_size, hidden_sizes[0])  # 2 for bidirection \n",
        "        self.BN = torch.nn.BatchNorm1d(hidden_sizes[0])\n",
        "        self.fc1 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.BN1 = torch.nn.BatchNorm1d(hidden_sizes[1])\n",
        "        self.fc2 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.BN2 = torch.nn.BatchNorm1d(hidden_sizes[2])\n",
        "        self.fc3_mu = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.fc3_sig = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        \n",
        "        if args_dict[\"dataset\"]==\"struc\":\n",
        "            self.fc4 = torch.nn.Linear(hidden_sizes[3]+1273, hidden_sizes[2])\n",
        "        else:        \n",
        "            self.fc4 = torch.nn.Linear(hidden_sizes[3]+2, hidden_sizes[2])\n",
        "        self.BN4 = torch.nn.BatchNorm1d(hidden_sizes[2])\n",
        "        self.fc5 = torch.nn.Linear(hidden_sizes[2], hidden_sizes[1])\n",
        "        self.BN5 = torch.nn.BatchNorm1d(hidden_sizes[1])\n",
        "        self.fc6 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[0])\n",
        "        self.BN6 = torch.nn.BatchNorm1d(hidden_sizes[0])\n",
        "        if args_dict[\"dataset\"]==\"struc\":\n",
        "            self.fc7 = torch.nn.Linear(hidden_sizes[0], input_size-1273)\n",
        "        else:\n",
        "            self.fc7 = torch.nn.Linear(hidden_sizes[0], input_size-2)\n",
        "            \n",
        "    def sample_z(self, mu, log_var):\n",
        "        # Using reparameterization trick to sample from a gaussian\n",
        "        \n",
        "        if cuda:\n",
        "            eps = torch.randn(self.batch_size, self.hidden_sizes[-1]).cuda()\n",
        "        else:\n",
        "            eps = torch.randn(self.batch_size, self.hidden_sizes[-1])\n",
        "\t\n",
        "        return mu + torch.exp(log_var / 2) * eps\n",
        "    \n",
        "    def forward(self, x, code, struc=None):\n",
        "        \n",
        "        ###########\n",
        "        # Encoder #\n",
        "        ###########\n",
        "        \n",
        "        # get the code from the tensor\n",
        "        # add the conditioned code\n",
        "        if args_dict[\"dataset\"]!=\"struc\":\n",
        "            x = torch.cat((x,code),1)\n",
        "        else:\n",
        "            x = torch.cat((x,code,struc),1)        \n",
        "        # Layer 0\n",
        "        out1 = self.fc(x)        \n",
        "        out1 = nn.relu(self.BN(out1))\n",
        "        # Layer 1\n",
        "        out2 = self.fc1(out1)\n",
        "        out2 = nn.relu(self.BN1(out2))\n",
        "        # Layer 2\n",
        "        out3 = self.fc2(out2)\n",
        "        out3 = nn.relu(self.BN2(out3))\n",
        "        # Layer 3 - mu\n",
        "        mu   = self.fc3_mu(out3)\n",
        "        # layer 3 - sig\n",
        "        sig  = nn.softplus(self.fc3_sig(out3))    \n",
        "\n",
        "         ###########\n",
        "        # Decoder #\n",
        "        ###########\n",
        "        \n",
        "        # sample from the distro\n",
        "        sample= self.sample_z(mu, sig)\n",
        "        # add the conditioned code\n",
        "        if args_dict[\"dataset\"]!=\"struc\": \n",
        "            sample = torch.cat((sample, code),1)\n",
        "        else:\n",
        "            sample = torch.cat((sample, code, struc),1)\n",
        "        # Layer 4\n",
        "        out4 = self.fc4(sample)\n",
        "        out4 = nn.relu(self.BN4(out4))\n",
        "        # Layer 5\n",
        "        out5 = self.fc5(out4)\n",
        "        out5 = nn.relu(self.BN5(out5))\n",
        "        # Layer 6\n",
        "        out6 = self.fc6(out5)\n",
        "        out6 = nn.relu(self.BN6(out6))\n",
        "        # Layer 7\n",
        "        out7 = nn.sigmoid(self.fc7(out6))\n",
        "        \n",
        "        return out7, mu, sig\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPPYkLL10D8N",
        "colab_type": "text"
      },
      "source": [
        "#Train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0B_l1VE0Fxd",
        "colab_type": "code",
        "outputId": "70417f0f-a70c-4e01-f828-e70aa66ff9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# =============================================================================\n",
        "# Training\n",
        "# =============================================================================\n",
        "\n",
        "# init the networks\n",
        "if cuda:\n",
        "    ff = feed_forward(X_dim, hidden_size, batch_size).cuda()\n",
        "else:\n",
        "    ff = feed_forward(X_dim, hidden_size, batch_size)\n",
        "\n",
        "# change the loading bit here\n",
        "if load:\n",
        "    ff.load_state_dict(torch.load(\"models/metal16_nostruc\", map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "# Loss and Optimizer\n",
        "solver = optim.Adam(ff.parameters(), lr=lr)\n",
        "burn_in_counter = 0\n",
        "tick = 0\n",
        "\n",
        "\n",
        "# number of epochs\n",
        "num_epochs=args_dict['num_epochs']\n",
        "# number of epochs\n",
        "num_epochs=args_dict['num_epochs']\n",
        "\n",
        "if train:\n",
        "    for its in range(num_epochs):\n",
        "        \n",
        "        #############################\n",
        "        # TRAINING \n",
        "        #############################\n",
        "        \n",
        "        ff.train()\n",
        "        scores=[]\n",
        "        data=shuffle(data)\n",
        "        print(\"Grammar Cond. - Epoch: {0}/{1}  Latent: {2}\".format(its,num_epochs,hidden_size[-1]))\n",
        "        for it in range(n // batch_size):\n",
        "        \n",
        "            if args_dict[\"dataset\"]==\"nostruc\":\n",
        "                \n",
        "                x_batch=data[it * batch_size: (it + 1) * batch_size]\n",
        "                code = x_batch[:,-2:]\n",
        "                x_batch = x_batch[:,:protein_dim]\n",
        "\n",
        "                if cuda:\n",
        "                    X = torch.from_numpy(x_batch).cuda().type(torch.cuda.FloatTensor)\n",
        "                    C = torch.from_numpy(code).cuda().type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                    X = torch.from_numpy(x_batch).type(torch.FloatTensor)\n",
        "                    C = torch.from_numpy(code).type(torch.FloatTensor)\n",
        "\n",
        "                \n",
        "            else:\n",
        "                x_batch=data[it * batch_size: (it + 1) * batch_size]\n",
        "                code = x_batch[:,-2:]\n",
        "                structure = x_batch[:,protein_dim:-2]\n",
        "                x_batch = x_batch[:,:protein_dim]\n",
        "\n",
        "                if cuda:\n",
        "                    X = torch.from_numpy(x_batch).cuda().type(torch.cuda.FloatTensor)\n",
        "                    C = torch.from_numpy(code).cuda().type(torch.cuda.FloatTensor)\n",
        "                    S = torch.from_numpy(structure).cuda().type(torch.cuda.FloatTensor) \n",
        "                else:\n",
        "                    X = torch.from_numpy(x_batch).type(torch.FloatTensor)\n",
        "                    C = torch.from_numpy(code).type(torch.FloatTensor)\n",
        "                    S = torch.from_numpy(structure).type(torch.FloatTensor)  \n",
        "    \n",
        "\n",
        "            \n",
        "            #turf last gradients\n",
        "            solver.zero_grad()\n",
        "            \n",
        "            \n",
        "            if args_dict[\"dataset\"]==\"struc\":\n",
        "            # Forward\n",
        "                x_sample, z_mu, z_var = ff(X, C, S)\n",
        "            else:\n",
        "                x_sample, z_mu, z_var = ff(X, C)\n",
        "            \n",
        "    \n",
        "                \n",
        "            # Loss\n",
        "            recon_loss = nn.binary_cross_entropy(x_sample, X, size_average=False) # by setting to false it sums instead of avg.\n",
        "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var)\n",
        "            #kl_loss=KL_Div(z_mu,z_var,unit_gauss=True,cuda=True)\n",
        "            kl_loss = kl_loss*burn_in_counter\n",
        "            loss = recon_loss + kl_loss\n",
        "            \n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "        \n",
        "            # Update\n",
        "            solver.step()\n",
        "            \n",
        "            \n",
        "            \n",
        "            len_aa=protein_dim\n",
        "            y_label=np.argmax(x_batch[:,:len_aa].reshape(batch_size,-1,22), axis=2)\n",
        "            y_pred =np.argmax(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22), axis=2)\n",
        "            \n",
        "\n",
        "            \n",
        "            # can use argmax again for clipping as it uses the first instance of 21\n",
        "            # loop with 256 examples is only about 3 milliseconds                      \n",
        "            for idx, row in enumerate(y_label):\n",
        "                scores.append(accuracy_score(row[:np.argmax(row)],y_pred[idx][:np.argmax(row)]))\n",
        "        \n",
        "        print(\"Tra Acc: {0}\".format(np.mean(scores)))\n",
        "                \n",
        "        if its==(num_epochs-1):\n",
        "            with open('latent_results_'+str(args_dict[\"dataset\"])+'.txt', 'a') as f:\n",
        "                f.write(str(args_dict['latent_dim'])+' train '+str(np.mean(scores)))\n",
        "\n",
        "\n",
        "        \n",
        "        if its>400 and burn_in_counter<1.0:\n",
        "            burn_in_counter+=0.003\n",
        "        \n",
        "        \n",
        "        \n",
        "        #############################\n",
        "        # Validation \n",
        "        #############################\n",
        "        \n",
        "        scores=[]\n",
        "        \n",
        "        ff.eval()\n",
        "        for it in range(data_test.shape[0] // batch_size):\n",
        "            x_batch=data_test[it * batch_size: (it + 1) * batch_size]\n",
        "\n",
        "            if args_dict[\"dataset\"]==\"nostruc\":\n",
        "\n",
        "                x_batch=data[it * batch_size: (it + 1) * batch_size]\n",
        "                code = x_batch[:,-2:]\n",
        "                x_batch = x_batch[:,:protein_dim]\n",
        "\n",
        "                if cuda:\n",
        "                    X = torch.from_numpy(x_batch).cuda().type(torch.cuda.FloatTensor)\n",
        "                    C = torch.from_numpy(code).cuda().type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                    X = torch.from_numpy(x_batch).type(torch.FloatTensor)\n",
        "                    C = torch.from_numpy(code).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "            else:\n",
        "                \n",
        "                x_batch=data[it * batch_size: (it + 1) * batch_size]\n",
        "                code = x_batch[:,-2:]\n",
        "                structure = x_batch[:,protein_dim:-2]\n",
        "                x_batch = x_batch[:,:protein_dim]\n",
        "\n",
        "                if cuda:\n",
        "                    X = torch.from_numpy(x_batch).cuda().type(torch.cuda.FloatTensor)\n",
        "                    C = torch.from_numpy(code).cuda().type(torch.cuda.FloatTensor)\n",
        "                    S = torch.from_numpy(structure).cuda().type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                    X = torch.from_numpy(x_batch).type(torch.FloatTensor)\n",
        "                    C = torch.from_numpy(code).type(torch.FloatTensor)\n",
        "                    S = torch.from_numpy(structure).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "            if args_dict[\"dataset\"]==\"struc\":\n",
        "            # Forward\n",
        "                x_sample, z_mu, z_var = ff(X, C, S)\n",
        "            else:\n",
        "                x_sample, z_mu, z_var = ff(X, C)\n",
        "\n",
        "            # Validation Loss\n",
        "            recon_loss = nn.binary_cross_entropy(x_sample, X, size_average=False) # by setting to false it sums instead of avg.\n",
        "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var)\n",
        "            #kl_loss=KL_Div(z_mu,z_var,unit_gauss=True,cuda=True)\n",
        "            kl_loss = kl_loss*burn_in_counter\n",
        "            loss = recon_loss + kl_loss\n",
        "            print(f\"Validation total loss {loss}\")\n",
        "\n",
        "        \n",
        "            len_aa=protein_dim\n",
        "            y_label=np.argmax(x_batch[:,:len_aa].reshape(batch_size,-1,22), axis=2)\n",
        "            y_pred =np.argmax(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22), axis=2)\n",
        "\n",
        "            for idx, row in enumerate(y_label):\n",
        "                l = row[:np.argmax(row)]\n",
        "                p = y_pred[idx][:np.argmax(row)]\n",
        "                acc = accuracy_score(row[:np.argmax(row)],y_pred[idx][:np.argmax(row)])\n",
        "                scores.append(acc)\n",
        "\n",
        "\n",
        "        print(\"Val Acc: {0}\".format(np.mean(scores)))\n",
        "\n",
        "        if its==(num_epochs-1):\n",
        "            with open('latent_results_'+str(args_dict[\"dataset\"])+'.txt', 'a') as f:\n",
        "                f.write(str(args_dict['latent_dim'])+' test '+str(np.mean(scores)))\n",
        "\n",
        "\n",
        "\n",
        "# saves if its running on gpu          \n",
        "import time\n",
        "if cuda:\n",
        "    torch.save(ff.state_dict(), '/content/drive/My Drive/model/metal'+str(args_dict['latent_dim'])+\"_\"\n",
        "    +str(args_dict['dataset'] + str(time.time())))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grammar Cond. - Epoch: 0/1400  Latent: 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tra Acc: 0.1497058922389787\n",
            "Validation total loss 156960.84375\n",
            "Validation total loss 159099.6875\n",
            "Validation total loss 157396.0625\n",
            "Validation total loss 147745.75\n",
            "Validation total loss 158539.59375\n",
            "Validation total loss 155684.109375\n",
            "Validation total loss 156140.34375\n",
            "Validation total loss 155462.84375\n",
            "Validation total loss 158825.203125\n",
            "Validation total loss 158937.1875\n",
            "Validation total loss 156228.1875\n",
            "Validation total loss 157452.203125\n",
            "Validation total loss 158791.4375\n",
            "Validation total loss 157191.140625\n",
            "Validation total loss 157919.265625\n",
            "Validation total loss 156324.15625\n",
            "Validation total loss 156425.78125\n",
            "Validation total loss 158370.734375\n",
            "Validation total loss 155334.796875\n",
            "Validation total loss 155786.984375\n",
            "Validation total loss 155865.21875\n",
            "Validation total loss 157049.09375\n",
            "Validation total loss 158956.84375\n",
            "Validation total loss 152851.25\n",
            "Val Acc: 0.21269260779158636\n",
            "Grammar Cond. - Epoch: 1/1400  Latent: 16\n",
            "Tra Acc: 0.23584298802708348\n",
            "Validation total loss 143946.3125\n",
            "Validation total loss 152939.46875\n",
            "Validation total loss 148782.078125\n",
            "Validation total loss 150573.71875\n",
            "Validation total loss 148036.5625\n",
            "Validation total loss 142874.84375\n",
            "Validation total loss 147707.15625\n",
            "Validation total loss 147552.9375\n",
            "Validation total loss 145384.734375\n",
            "Validation total loss 144003.4375\n",
            "Validation total loss 148661.859375\n",
            "Validation total loss 146401.46875\n",
            "Validation total loss 148464.671875\n",
            "Validation total loss 145581.828125\n",
            "Validation total loss 149490.296875\n",
            "Validation total loss 148574.0625\n",
            "Validation total loss 148818.5\n",
            "Validation total loss 141874.5\n",
            "Validation total loss 143908.203125\n",
            "Validation total loss 148299.8125\n",
            "Validation total loss 145819.984375\n",
            "Validation total loss 147731.5625\n",
            "Validation total loss 146489.75\n",
            "Validation total loss 149697.765625\n",
            "Val Acc: 0.25100082511129956\n",
            "Grammar Cond. - Epoch: 2/1400  Latent: 16\n",
            "Tra Acc: 0.26939351842739034\n",
            "Validation total loss 135475.015625\n",
            "Validation total loss 137339.453125\n",
            "Validation total loss 137991.375\n",
            "Validation total loss 142690.515625\n",
            "Validation total loss 140082.203125\n",
            "Validation total loss 138211.40625\n",
            "Validation total loss 142466.203125\n",
            "Validation total loss 140117.84375\n",
            "Validation total loss 139695.28125\n",
            "Validation total loss 136110.46875\n",
            "Validation total loss 141377.578125\n",
            "Validation total loss 142183.609375\n",
            "Validation total loss 136999.1875\n",
            "Validation total loss 133830.875\n",
            "Validation total loss 133723.9375\n",
            "Validation total loss 137043.515625\n",
            "Validation total loss 144924.6875\n",
            "Validation total loss 140858.5625\n",
            "Validation total loss 135812.625\n",
            "Validation total loss 140636.09375\n",
            "Validation total loss 135431.28125\n",
            "Validation total loss 137577.4375\n",
            "Validation total loss 141200.46875\n",
            "Validation total loss 140170.9375\n",
            "Val Acc: 0.2895306010978378\n",
            "Grammar Cond. - Epoch: 3/1400  Latent: 16\n",
            "Tra Acc: 0.3006507689124307\n",
            "Validation total loss 133321.734375\n",
            "Validation total loss 137531.375\n",
            "Validation total loss 135273.1875\n",
            "Validation total loss 135329.140625\n",
            "Validation total loss 133537.1875\n",
            "Validation total loss 136469.3125\n",
            "Validation total loss 136676.625\n",
            "Validation total loss 133478.640625\n",
            "Validation total loss 131917.96875\n",
            "Validation total loss 135735.890625\n",
            "Validation total loss 132626.046875\n",
            "Validation total loss 129575.296875\n",
            "Validation total loss 132039.484375\n",
            "Validation total loss 135481.03125\n",
            "Validation total loss 136990.421875\n",
            "Validation total loss 130644.9296875\n",
            "Validation total loss 128183.0390625\n",
            "Validation total loss 133700.390625\n",
            "Validation total loss 129799.9921875\n",
            "Validation total loss 133198.921875\n",
            "Validation total loss 137089.6875\n",
            "Validation total loss 134692.0625\n",
            "Validation total loss 135373.515625\n",
            "Validation total loss 134497.609375\n",
            "Val Acc: 0.3162875364632601\n",
            "Grammar Cond. - Epoch: 4/1400  Latent: 16\n",
            "Tra Acc: 0.3282362587138283\n",
            "Validation total loss 127849.5625\n",
            "Validation total loss 128093.046875\n",
            "Validation total loss 133258.78125\n",
            "Validation total loss 128765.265625\n",
            "Validation total loss 128978.84375\n",
            "Validation total loss 131607.671875\n",
            "Validation total loss 122919.765625\n",
            "Validation total loss 128000.984375\n",
            "Validation total loss 127784.0703125\n",
            "Validation total loss 129157.3984375\n",
            "Validation total loss 126130.0078125\n",
            "Validation total loss 128740.4140625\n",
            "Validation total loss 124395.6875\n",
            "Validation total loss 124132.0859375\n",
            "Validation total loss 130448.984375\n",
            "Validation total loss 127444.984375\n",
            "Validation total loss 125990.484375\n",
            "Validation total loss 126842.34375\n",
            "Validation total loss 133706.296875\n",
            "Validation total loss 132010.25\n",
            "Validation total loss 130647.7734375\n",
            "Validation total loss 132148.421875\n",
            "Validation total loss 130484.1640625\n",
            "Validation total loss 127808.546875\n",
            "Val Acc: 0.34337836962999263\n",
            "Grammar Cond. - Epoch: 5/1400  Latent: 16\n",
            "Tra Acc: 0.3506242744367725\n",
            "Validation total loss 122055.171875\n",
            "Validation total loss 124064.6796875\n",
            "Validation total loss 122189.5703125\n",
            "Validation total loss 120884.03125\n",
            "Validation total loss 120186.3828125\n",
            "Validation total loss 125286.8828125\n",
            "Validation total loss 125658.15625\n",
            "Validation total loss 122670.0703125\n",
            "Validation total loss 124781.0859375\n",
            "Validation total loss 130463.09375\n",
            "Validation total loss 122784.9453125\n",
            "Validation total loss 124900.4296875\n",
            "Validation total loss 127186.4921875\n",
            "Validation total loss 123639.09375\n",
            "Validation total loss 132654.890625\n",
            "Validation total loss 129653.515625\n",
            "Validation total loss 127533.2734375\n",
            "Validation total loss 124603.9921875\n",
            "Validation total loss 124504.9453125\n",
            "Validation total loss 125671.71875\n",
            "Validation total loss 124233.546875\n",
            "Validation total loss 130150.96875\n",
            "Validation total loss 129685.65625\n",
            "Validation total loss 126946.8828125\n",
            "Val Acc: 0.360980881596042\n",
            "Grammar Cond. - Epoch: 6/1400  Latent: 16\n",
            "Tra Acc: 0.3698270981681937\n",
            "Validation total loss 118474.265625\n",
            "Validation total loss 120356.4296875\n",
            "Validation total loss 122245.703125\n",
            "Validation total loss 122820.1484375\n",
            "Validation total loss 120529.8125\n",
            "Validation total loss 122496.8828125\n",
            "Validation total loss 124472.8671875\n",
            "Validation total loss 118845.3125\n",
            "Validation total loss 123465.8046875\n",
            "Validation total loss 124543.28125\n",
            "Validation total loss 122132.6875\n",
            "Validation total loss 117719.0\n",
            "Validation total loss 121704.6328125\n",
            "Validation total loss 123878.6171875\n",
            "Validation total loss 124722.125\n",
            "Validation total loss 117574.5625\n",
            "Validation total loss 121819.21875\n",
            "Validation total loss 121633.171875\n",
            "Validation total loss 120406.78125\n",
            "Validation total loss 123644.515625\n",
            "Validation total loss 123812.0859375\n",
            "Validation total loss 121561.984375\n",
            "Validation total loss 122828.2578125\n",
            "Validation total loss 118581.875\n",
            "Val Acc: 0.379022812922462\n",
            "Grammar Cond. - Epoch: 7/1400  Latent: 16\n",
            "Tra Acc: 0.3875031794343962\n",
            "Validation total loss 118823.703125\n",
            "Validation total loss 119206.2734375\n",
            "Validation total loss 120406.59375\n",
            "Validation total loss 114673.3046875\n",
            "Validation total loss 122267.3984375\n",
            "Validation total loss 120959.2421875\n",
            "Validation total loss 117413.3046875\n",
            "Validation total loss 113424.765625\n",
            "Validation total loss 121443.984375\n",
            "Validation total loss 114602.34375\n",
            "Validation total loss 116762.53125\n",
            "Validation total loss 114700.5625\n",
            "Validation total loss 125512.7890625\n",
            "Validation total loss 121499.375\n",
            "Validation total loss 114621.2421875\n",
            "Validation total loss 120698.3125\n",
            "Validation total loss 113924.0859375\n",
            "Validation total loss 118680.0625\n",
            "Validation total loss 115925.390625\n",
            "Validation total loss 119140.1875\n",
            "Validation total loss 120255.9375\n",
            "Validation total loss 119799.0\n",
            "Validation total loss 117392.609375\n",
            "Validation total loss 112281.2265625\n",
            "Val Acc: 0.3989228229611352\n",
            "Grammar Cond. - Epoch: 8/1400  Latent: 16\n",
            "Tra Acc: 0.4041161717694491\n",
            "Validation total loss 110726.6875\n",
            "Validation total loss 112188.25\n",
            "Validation total loss 111920.5546875\n",
            "Validation total loss 110980.375\n",
            "Validation total loss 117443.3203125\n",
            "Validation total loss 115948.0625\n",
            "Validation total loss 112526.53125\n",
            "Validation total loss 118856.984375\n",
            "Validation total loss 114205.546875\n",
            "Validation total loss 114216.171875\n",
            "Validation total loss 111575.890625\n",
            "Validation total loss 110605.484375\n",
            "Validation total loss 111623.328125\n",
            "Validation total loss 120380.6328125\n",
            "Validation total loss 115807.078125\n",
            "Validation total loss 114498.2265625\n",
            "Validation total loss 120685.7734375\n",
            "Validation total loss 116547.1015625\n",
            "Validation total loss 118376.140625\n",
            "Validation total loss 109168.9609375\n",
            "Validation total loss 109227.625\n",
            "Validation total loss 116162.703125\n",
            "Validation total loss 117330.015625\n",
            "Validation total loss 113903.140625\n",
            "Val Acc: 0.42070214292817826\n",
            "Grammar Cond. - Epoch: 9/1400  Latent: 16\n",
            "Tra Acc: 0.41845765377683924\n",
            "Validation total loss 121760.859375\n",
            "Validation total loss 113455.171875\n",
            "Validation total loss 113189.7578125\n",
            "Validation total loss 108103.546875\n",
            "Validation total loss 111787.421875\n",
            "Validation total loss 114624.140625\n",
            "Validation total loss 115709.171875\n",
            "Validation total loss 115798.828125\n",
            "Validation total loss 116139.6328125\n",
            "Validation total loss 110387.453125\n",
            "Validation total loss 114139.0\n",
            "Validation total loss 117027.71875\n",
            "Validation total loss 111300.5546875\n",
            "Validation total loss 112765.7421875\n",
            "Validation total loss 114109.296875\n",
            "Validation total loss 117180.15625\n",
            "Validation total loss 113677.28125\n",
            "Validation total loss 118314.109375\n",
            "Validation total loss 106699.015625\n",
            "Validation total loss 116875.8828125\n",
            "Validation total loss 114545.953125\n",
            "Validation total loss 113951.8984375\n",
            "Validation total loss 110709.9921875\n",
            "Validation total loss 112394.2421875\n",
            "Val Acc: 0.42339249665489936\n",
            "Grammar Cond. - Epoch: 10/1400  Latent: 16\n",
            "Tra Acc: 0.4315309040223133\n",
            "Validation total loss 108686.234375\n",
            "Validation total loss 111558.015625\n",
            "Validation total loss 110608.0859375\n",
            "Validation total loss 106284.1953125\n",
            "Validation total loss 101915.4375\n",
            "Validation total loss 116424.375\n",
            "Validation total loss 107241.703125\n",
            "Validation total loss 115253.203125\n",
            "Validation total loss 111139.8828125\n",
            "Validation total loss 106054.6953125\n",
            "Validation total loss 109338.6640625\n",
            "Validation total loss 106122.40625\n",
            "Validation total loss 108002.0703125\n",
            "Validation total loss 109789.7265625\n",
            "Validation total loss 111642.8125\n",
            "Validation total loss 109634.5390625\n",
            "Validation total loss 110107.6328125\n",
            "Validation total loss 102958.28125\n",
            "Validation total loss 115234.421875\n",
            "Validation total loss 107875.84375\n",
            "Validation total loss 112349.15625\n",
            "Validation total loss 108027.3046875\n",
            "Validation total loss 105824.46875\n",
            "Validation total loss 112988.8984375\n",
            "Val Acc: 0.4482613489888212\n",
            "Grammar Cond. - Epoch: 11/1400  Latent: 16\n",
            "Tra Acc: 0.4421769777040135\n",
            "Validation total loss 104543.546875\n",
            "Validation total loss 101280.296875\n",
            "Validation total loss 101298.75\n",
            "Validation total loss 108355.5078125\n",
            "Validation total loss 108567.171875\n",
            "Validation total loss 114396.8046875\n",
            "Validation total loss 107464.6875\n",
            "Validation total loss 112223.203125\n",
            "Validation total loss 104062.375\n",
            "Validation total loss 105522.2890625\n",
            "Validation total loss 109870.96875\n",
            "Validation total loss 102317.921875\n",
            "Validation total loss 112395.8125\n",
            "Validation total loss 110691.21875\n",
            "Validation total loss 110068.609375\n",
            "Validation total loss 109201.828125\n",
            "Validation total loss 106797.46875\n",
            "Validation total loss 105701.890625\n",
            "Validation total loss 107178.765625\n",
            "Validation total loss 105240.7578125\n",
            "Validation total loss 108209.40625\n",
            "Validation total loss 117048.984375\n",
            "Validation total loss 105708.0625\n",
            "Validation total loss 110074.8125\n",
            "Val Acc: 0.45428732202271355\n",
            "Grammar Cond. - Epoch: 12/1400  Latent: 16\n",
            "Tra Acc: 0.4515525909869375\n",
            "Validation total loss 103239.8515625\n",
            "Validation total loss 107998.0078125\n",
            "Validation total loss 103975.46875\n",
            "Validation total loss 100413.5546875\n",
            "Validation total loss 105715.9921875\n",
            "Validation total loss 108148.2890625\n",
            "Validation total loss 109823.2421875\n",
            "Validation total loss 110811.328125\n",
            "Validation total loss 104963.96875\n",
            "Validation total loss 110149.1328125\n",
            "Validation total loss 108155.484375\n",
            "Validation total loss 110844.03125\n",
            "Validation total loss 114763.8828125\n",
            "Validation total loss 105493.3203125\n",
            "Validation total loss 108472.140625\n",
            "Validation total loss 108993.9453125\n",
            "Validation total loss 100787.3125\n",
            "Validation total loss 107634.125\n",
            "Validation total loss 110388.640625\n",
            "Validation total loss 106670.59375\n",
            "Validation total loss 108025.828125\n",
            "Validation total loss 110516.359375\n",
            "Validation total loss 103339.78125\n",
            "Validation total loss 101079.734375\n",
            "Val Acc: 0.45898449361099397\n",
            "Grammar Cond. - Epoch: 13/1400  Latent: 16\n",
            "Tra Acc: 0.46082167956459097\n",
            "Validation total loss 110985.484375\n",
            "Validation total loss 104851.0\n",
            "Validation total loss 105175.4296875\n",
            "Validation total loss 104092.578125\n",
            "Validation total loss 103841.609375\n",
            "Validation total loss 101839.015625\n",
            "Validation total loss 107195.765625\n",
            "Validation total loss 107801.8515625\n",
            "Validation total loss 102629.703125\n",
            "Validation total loss 108543.1171875\n",
            "Validation total loss 103725.1953125\n",
            "Validation total loss 104443.3359375\n",
            "Validation total loss 102171.59375\n",
            "Validation total loss 103883.390625\n",
            "Validation total loss 103977.6640625\n",
            "Validation total loss 102462.953125\n",
            "Validation total loss 107395.625\n",
            "Validation total loss 107985.953125\n",
            "Validation total loss 105806.03125\n",
            "Validation total loss 103925.671875\n",
            "Validation total loss 104748.015625\n",
            "Validation total loss 103877.1328125\n",
            "Validation total loss 110449.546875\n",
            "Validation total loss 97839.1875\n",
            "Val Acc: 0.4689714341702549\n",
            "Grammar Cond. - Epoch: 14/1400  Latent: 16\n",
            "Tra Acc: 0.4691919271344392\n",
            "Validation total loss 101714.5234375\n",
            "Validation total loss 98478.71875\n",
            "Validation total loss 112418.7421875\n",
            "Validation total loss 103533.3046875\n",
            "Validation total loss 99552.65625\n",
            "Validation total loss 100078.3046875\n",
            "Validation total loss 101650.953125\n",
            "Validation total loss 99709.5078125\n",
            "Validation total loss 104719.75\n",
            "Validation total loss 107436.0703125\n",
            "Validation total loss 104805.046875\n",
            "Validation total loss 101633.1875\n",
            "Validation total loss 103674.5703125\n",
            "Validation total loss 101187.46875\n",
            "Validation total loss 98440.609375\n",
            "Validation total loss 107309.609375\n",
            "Validation total loss 102214.1875\n",
            "Validation total loss 111294.9296875\n",
            "Validation total loss 104651.71875\n",
            "Validation total loss 106268.765625\n",
            "Validation total loss 107563.828125\n",
            "Validation total loss 105021.3984375\n",
            "Validation total loss 103367.1875\n",
            "Validation total loss 108262.890625\n",
            "Val Acc: 0.471165122507486\n",
            "Grammar Cond. - Epoch: 15/1400  Latent: 16\n",
            "Tra Acc: 0.4771997550787862\n",
            "Validation total loss 103275.484375\n",
            "Validation total loss 98569.234375\n",
            "Validation total loss 101869.7265625\n",
            "Validation total loss 94439.515625\n",
            "Validation total loss 99856.625\n",
            "Validation total loss 107814.3984375\n",
            "Validation total loss 98450.21875\n",
            "Validation total loss 100929.265625\n",
            "Validation total loss 106785.734375\n",
            "Validation total loss 100376.234375\n",
            "Validation total loss 97928.9375\n",
            "Validation total loss 102639.015625\n",
            "Validation total loss 105586.3125\n",
            "Validation total loss 101315.703125\n",
            "Validation total loss 99367.0234375\n",
            "Validation total loss 106786.203125\n",
            "Validation total loss 98703.234375\n",
            "Validation total loss 95854.53125\n",
            "Validation total loss 100728.5859375\n",
            "Validation total loss 97906.3203125\n",
            "Validation total loss 103243.5078125\n",
            "Validation total loss 106204.3125\n",
            "Validation total loss 107786.9140625\n",
            "Validation total loss 103986.578125\n",
            "Val Acc: 0.4858477391026717\n",
            "Grammar Cond. - Epoch: 16/1400  Latent: 16\n",
            "Tra Acc: 0.4848362724857767\n",
            "Validation total loss 104980.5859375\n",
            "Validation total loss 100060.875\n",
            "Validation total loss 102674.78125\n",
            "Validation total loss 96298.6796875\n",
            "Validation total loss 96326.0859375\n",
            "Validation total loss 101454.390625\n",
            "Validation total loss 99289.2421875\n",
            "Validation total loss 98391.9375\n",
            "Validation total loss 102227.203125\n",
            "Validation total loss 94192.25\n",
            "Validation total loss 92821.5\n",
            "Validation total loss 99512.296875\n",
            "Validation total loss 100186.5546875\n",
            "Validation total loss 101508.8359375\n",
            "Validation total loss 94971.359375\n",
            "Validation total loss 103400.2578125\n",
            "Validation total loss 100061.453125\n",
            "Validation total loss 104510.8125\n",
            "Validation total loss 90950.6015625\n",
            "Validation total loss 100732.25\n",
            "Validation total loss 95127.453125\n",
            "Validation total loss 99652.578125\n",
            "Validation total loss 105343.5\n",
            "Validation total loss 95946.75\n",
            "Val Acc: 0.49458359435892657\n",
            "Grammar Cond. - Epoch: 17/1400  Latent: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD7TYtRB0gT5",
        "colab_type": "text"
      },
      "source": [
        "# Launch model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BUD-H0A0jic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newMetalBinder(model,data,name):\n",
        "    \"\"\"\n",
        "    Generates a new sequence based on a metal code and a grammar. \n",
        "    The data array is (4353,) where the first 3080 are the\n",
        "    sequence, the next 1265 are the fold and the final 8 are the metal\n",
        "    binding flags. Fold is optional\n",
        "    \"\"\"\n",
        "    scores=[]\n",
        "    #model into eval mode    \n",
        "    model.eval()\n",
        "    \n",
        "    if args_dict[\"dataset\"]==\"nostruc\":\n",
        "\n",
        "        code = np.tile(data[-8:],(model.batch_size,1))\n",
        "        x = np.tile(data[:protein_dim],(model.batch_size,1))\n",
        "\n",
        "        if cuda:\n",
        "            X = torch.from_numpy(x).cuda().type(torch.cuda.FloatTensor)\n",
        "            C = torch.from_numpy(code).cuda().type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "            X = torch.from_numpy(x).type(torch.FloatTensor)\n",
        "            C = torch.from_numpy(code).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "    else:\n",
        "        \n",
        "        code = np.tile(data[-8:],(model.batch_size,1))\n",
        "        structure = np.tile(data[protein_dim:-8],(model.batch_size,1))\n",
        "        x = np.tile(data[:protein_dim],(model.batch_size,1))\n",
        "\n",
        "        if cuda:\n",
        "            X = torch.from_numpy(x).cuda().type(torch.cuda.FloatTensor)\n",
        "            C = torch.from_numpy(code).cuda().type(torch.cuda.FloatTensor)\n",
        "            S = torch.from_numpy(structure).cuda().type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "            X = torch.from_numpy(x).type(torch.FloatTensor)\n",
        "            C = torch.from_numpy(code).type(torch.FloatTensor)\n",
        "            S = torch.from_numpy(structure).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "    if args_dict[\"dataset\"]==\"struc\":\n",
        "    # Forward\n",
        "        x_sample, z_mu, z_var = ff(X, C, S)\n",
        "    else:\n",
        "        x_sample, z_mu, z_var = ff(X, C)\n",
        "    \n",
        "    \n",
        "    len_aa=140*22\n",
        "    y_label=np.argmax(x[:,:len_aa].reshape(batch_size,-1,22), axis=2)\n",
        "    y_pred =np.argmax(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22), axis=2)\n",
        "    #np.save(name,y_pred)\n",
        "    print(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22).shape)\n",
        "    np.save(name,x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22))\n",
        "    \n",
        "    \n",
        "    for idx, row in enumerate(y_label):\n",
        "        scores.append(accuracy_score(row[:np.argmax(row)],y_pred[idx][:np.argmax(row)]))\n",
        "    print(\"Accuracy: {0}\".format(np.mean(scores)))\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}